{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bite5de813d2dd14b7eb020a30a16f1757f",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# SpaCy\n",
    "import spacy \n",
    "# python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fa299bcdd10>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0) set seed for reproduciability\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/lefko/personal/deep-learning/data/'\n",
    "\n",
    "def loadTrain():\n",
    "    return ''.join([data_dir, 'train.csv'])\n",
    "\n",
    "def loadTest():\n",
    "    return ''.join([data_dir, 'test.csv'])\n",
    "\n",
    "#print('path to train.csv:', ''.join([data_dir, 'train.csv']))\n",
    "train_df = pd.read_csv(loadTrain())\n",
    "test_df = pd.read_csv(loadTest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n5       1  \n6       1  \n7       1  \n8       1  \n9       1  "
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7613"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3342 unique locations found \n\n222 unique keywords found \n\n101.03743596479706 average length of a tweet \n\n"
    }
   ],
   "source": [
    "# How many unique locations?\n",
    "print(len(train_df['location'].unique()), 'unique locations found \\n')\n",
    "# How many unique keywords?\n",
    "print(len(train_df['keyword'].unique()), 'unique keywords found \\n')\n",
    "# Average tweet length?\n",
    "print(np.mean(train_df['text'].str.len()), 'average length of a tweet \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "spacy.load('en')\n",
    "\n",
    "# declare tokenizers\n",
    "TEXT = data.Field(tokenize='spacy', batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first = True)\n",
    "\n",
    "# declare fields for the dataloader - PyTorch thing\n",
    "train_fields = [(None, None), (None, None), (None, None), ('text', TEXT), ('label', LABEL)] # (id), (keyword), (location), (text), (label)\n",
    "test_fields = [(None, None), (None, None), (None, None), ('text', TEXT)] # (id), (keyword), (location), (text)\n",
    "\n",
    "\n",
    "# custom PyTorch dataset\n",
    "train_loader = data.TabularDataset(path=loadTrain(), format='csv', fields=train_fields, skip_header=True) # labelled train/validation data\n",
    "test_loader = data.TabularDataset(path=loadTest(), format='csv', fields=test_fields, skip_header=True) # unlabelled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'text': ['Our', 'Deeds', 'are', 'the', 'Reason', 'of', 'this', '#', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us', 'all'], 'label': '1'}\n{'text': ['Just', 'happened', 'a', 'terrible', 'car', 'crash']}\n"
    }
   ],
   "source": [
    "print(vars(train_loader.examples[0]))\n",
    "print(vars(test_loader.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train-val (80:20)\n",
    "train_data, valid_data = train_loader.split(split_ratio=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": ".vector_cache/glove.6B.zip: 862MB [06:51, 2.10MB/s]\n100%|█████████▉| 399390/400000 [00:15<00:00, 24388.79it/s]"
    }
   ],
   "source": [
    "# build training vocabulary\n",
    "min_word_freq = 3 # word must occure at least x times\n",
    "glove_ver = 'glove.6B.100d' # https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "TEXT.build_vocab(train_data, min_freq = min_word_freq, vectors = glove_ver) # build word vectors using pre-trained embeddings\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Size of TEXT vocabulary: 1471\nSize of LABEL vocabulary: 2\n[('#', 672), ('.', 601), ('?', 551), ('the', 514), (':', 386), ('a', 381), ('to', 376), ('in', 355), ('of', 334), ('I', 320)]\n"
    }
   ],
   "source": [
    "# show some statistics\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "#print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training batches\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()          \n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 1\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = True, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "classifier(\n  (embedding): Embedding(1471, 100)\n  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (fc): Linear(in_features=64, out_features=1, bias=True)\n  (act): Sigmoid()\n)\nThe model has 206,557 trainable parameters\ntorch.Size([1471, 100])\n"
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "#model = model.to(device)\n",
    "#criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze()  \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train Loss: 0.404 | Train Acc: 82.55%\n\t Val. Loss: 0.511 |  Val. Acc: 77.12%\n\tTrain Loss: 0.344 | Train Acc: 86.18%\n\t Val. Loss: 0.544 |  Val. Acc: 74.37%\n\tTrain Loss: 0.290 | Train Acc: 88.46%\n\t Val. Loss: 0.553 |  Val. Acc: 76.73%\n\tTrain Loss: 0.247 | Train Acc: 90.40%\n\t Val. Loss: 0.623 |  Val. Acc: 76.09%\n\tTrain Loss: 0.230 | Train Acc: 91.29%\n\t Val. Loss: 0.652 |  Val. Acc: 75.24%\n\tTrain Loss: 0.187 | Train Acc: 93.08%\n\t Val. Loss: 0.690 |  Val. Acc: 74.55%\n\tTrain Loss: 0.152 | Train Acc: 94.94%\n\t Val. Loss: 0.779 |  Val. Acc: 72.80%\n\tTrain Loss: 0.151 | Train Acc: 94.60%\n\t Val. Loss: 0.757 |  Val. Acc: 73.73%\n\tTrain Loss: 0.126 | Train Acc: 95.70%\n\t Val. Loss: 0.778 |  Val. Acc: 74.63%\n\tTrain Loss: 0.117 | Train Acc: 96.29%\n\t Val. Loss: 0.881 |  Val. Acc: 72.62%\n\tTrain Loss: 0.094 | Train Acc: 96.48%\n\t Val. Loss: 0.909 |  Val. Acc: 74.22%\n\tTrain Loss: 0.082 | Train Acc: 97.26%\n\t Val. Loss: 0.959 |  Val. Acc: 73.47%\n\tTrain Loss: 0.072 | Train Acc: 97.61%\n\t Val. Loss: 1.022 |  Val. Acc: 72.98%\n\tTrain Loss: 0.064 | Train Acc: 98.00%\n\t Val. Loss: 1.034 |  Val. Acc: 74.22%\n\tTrain Loss: 0.068 | Train Acc: 97.52%\n\t Val. Loss: 1.082 |  Val. Acc: 71.75%\n"
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights\n",
    "path='./saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "\n",
    "#inference \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict(model, sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
    "    length = [len(indexed)]                                    #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
    "    prediction = model(tensor, length_tensor)                  #prediction \n",
    "    return prediction.item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Just happened a terrible car crash'"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = vars(test_loader.examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.7229582667350769\n"
    }
   ],
   "source": [
    "print(predict(model, ' '.join(t1['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "your city in a Û _ http://t.co/S6Rc7dbAL1\nRT @calgarysun : Sun photographer Stuart Dryden spotted this vortex spun off a violent storm in Sylvan Lake . # abstorm http://t.co/w0BZ0JNGvB\nHoly crap ! BRAVO Sir ! Amazing ! Dramatic Video Shows Plane Landing During Violent Storm http://t.co/xB0bw8h8Ur\nViolent Forces Radio : Now Playing Torture - Storm Alert \n TuneIn Player @ http://t.co/XsSgEdSbH4\nCircus tent collapses in violent storm killing 2 kids in New Hampshire http://t.co/j0saXEKBTa\nStorm is here ! Violent winds and pounding rains in Evergreen . # yyc\nStay inside for the next little while kids . We 're having a bit of a violent storm right now . -kjc\nWatch : Violent Storm Causes Deadly Accident at New Hampshire Circus http://t.co/jpPiR3ZzKA # GMA http://t.co/nV5GCDpIBA\nViolent Forces Radio : Now Playing Axegressor - Psalm Before the Storm \n TuneIn Player @ http://t.co/XsSgEdSbH4\n# calgaryweather   It would be nice if they would fix radar before another violent storm   Uninformed citizens dangerous # YYC # yycweather\n2012#Shell 's 250-foot - tall drilling rig broke loose from a towline&amp;drifted out of control during violent winter storm # incompetent # shellno\nhttp://t.co/16EClWrW84 Asics GT - II Super Red 2.0 11 Ronnie Fieg Kith Red White 3 M x gel grey volcano 2\nForce Factor VOLCANO 120 Capsule Brand New   - Full read by eBay http://t.co/HewutDLDqh http://t.co/8etXXzBdua\nThe sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT\n@CatholicMomVA Or take away his ring & amp ; throw it in a volcano ...\nThe highlight of my day has been playing with my raw volcano humidifier . # sicklife http://t.co/teDRu4hg3i\n@Jesssssssee @eevans17 The Illuminati : The Yellowstone Super Volcano Conspiracy http://t.co/wymv9cYxzK\nidgaf fly into a volcano\n? ? @ KETEP VOLCANO CENTRE ketep PASS https://t.co/IdBnKxu80L\nLearning from the Legacy of a Catastrophic Eruption - The New Yorker http://t.co/y8YqPBE4t9\n# USGS M 1.9 - 5 km S of Volcano Hawaii : Time2015 - 08 - 06 01:04:01 UTC2015 - 08 - 05 15:04:01 -10:00 at epicenter ... http://t.co/meVvkaXcdE # SM\nCLUB GOING UP MIXTAPE ( RNB EDITION ) 2015 ? ? BY DJ VOLCANO X STARZ TEAM \n\n DOWNLOAD : https://t.co/DpBFuU2GD6\n# Sismo M 1.9 - 5 km S of Volcano Hawaii : Time2015 - 08 - 06 01:04:01 UTC2015 - 08 - 05 15:04:01 -10:00 at epicente ... http://t.co/K28mgWTZcV # CS\nQuake : M 1.9 - 5 km S of Volcano Hawaii\nwhat 's the hottest thing you 've ever seen .. fire? .. the sun? .. a volcano ?   or this ? http://t.co/2GLdsHRaiI\nThe Architect Behind Kanye West 's Volcano - New York Times http://t.co/3QJUExX0Y0\nJust watched Punisher : War Zone . IDK why everyone hates it . It is basically the best possible movie you could get out of Punisher\nMy new sounds : War Zone https://t.co/hNXRfqRk3P on # SoundCloud\nSammy is here in this war zone . Jamal spoke to me on the phone . Now my wife is next to speak to me ... what else can&gt ; http://t.co/2CppfprxoG\n@thelovatoagent omg i feel like i am in a war zone\nI think it 's time for a cup of tea and a scone before I tackle the war zone that is is my bedroom . I wonder how many condom packets Ill find\nSaipan looks like a war zone though\nT Shirts $ 10 male or female get wit me . 2 days until its game changin time . War Zone single will beÛ _ https://t.co/Z0poYR096J\nUm OK ' @dandre_5 : Sundays during football seasonfrom about 9 am - 11 pm women should n't even log onshit be a complete war zone '\nJust trying to find my my peace in a Zone of war\nTwitter is going to be a war zone today   https://t.co/1QHNqCZvod\nThe Resistance wages guerrilla warfare against KPA forces in the Red Zone the war - torn outskirts of ... http://t.co/WFxAwQvvjr\nWhen your friend getting whooped and you running out of weapon options ? ? ? ? ? ? # TeamDuval # T ... ( Vine by TerrellT ) https://t.co/Sf9Ufv7w6C\n3 Stars in 212 seconds ! Level 43 Surgeon . Weapon of choice : Neutron Rifle . # callofmini # doubleshot\nYo that guy changed his weapon mid battle how did he do that ?\nfun fact : every rapid - fire weapon in the game is a hard counter to the slosher and the splattling\nSlosher is a fucboi weapon\nNo more war . People need no any weapon . \n We need the world of peace . https://t.co/DhRvYtjlFL\n# AskConnor there 's a zombie apocalypse . the item to your right is your weapon . you 're either screwed or you 're gon na live .\nTrue strength is forgiveness . Love the most powerful weapon . .@vickysuewrites ' Broken Circle ' \n\n # giveaway # boyxboy http://t.co/Zgc3EsLNPS\nOn 70th anniversary of # Hiroshima bomb it is important to learn from history and ban this weapon .\n@KnaveTBE lol . Not the class but the class of weapon\n@sindy642498 \n I 'm really glad to hear thatÛ _ ; ; \n Nowlike usthe world is truely connected . \n I want truely peaceful world without any weapon !\n@JPens4Real21 Geno has weapons now I 'll give him the benefit of the doubt this year . I 'm just not a huge believer . Still not mature enough .\n@DANNYonPC @YouTube Its engineer only again so 2 1000 + rpm weapons = no thanks\nHelpful Tips For Nuking An Asteroid http://t.co/GwRsga9uJV\nI 'm all in favor of keeping dangerous weapons out of the hands of fools . Let 's start with typewriters .\nSecurity experts report that the only way to keep Iran ' honest ' and stop her march to nuclear weapons is by ... http://t.co/DGnbUsfXkw\nAug 3 1915ÛÓKILL 10000 WITH ROCKS . ; Italians Make Good Use of Nature 's Weapons -- Excel with Bayonet . \n http://t.co/ck1mMvHU0 T\n@deray @marstu67 Cop Benefits : Great Salary Free Car Free Rent Free Food . Free Weapons License to Kill & amp ; Rape with Impunity .\n? ? Water fight ? ? \n Penn park 6 pm \n       BYOW \n ( Bring Your Own Weapons )\nAbe proposed a new plan for abolishing nuclear weapons . Yesterday his defense minister made clear nukes are fine . http://t.co/A6IR8rKuwS\nNAVY SAYS LT CMDR WHO FIRED SIDEARM IN DEFENSE DURING NOSC CHATTANOOGA TERRORIST ATTACK WILL NOT FACE CHARGES ... http://t.co/vCcDKC4qt2\nAny attempt to sell weapons to # Nigeria is an attempt to upgrad # Bokoharm army # USA must know this now @UN\nWhirlwind full of bombarded kunais .\n@BarstoolBigCat what a whirlwind .\nI 've officially been a St. Louisan for one full year . What a crazy awesome overwhelming whirlwind year it has been . Here 's to another !\nMissing Malaysian airplane : Debris was planted ÛÒ Families News - WhirlWind News http://t.co/bLd0nHKzbh\nWarcraft has always been this weird referential whirlwind to itself and other things\n@Chelsea_Rosette ah bless ya . Not too bad . My life 's been a bit of a whirlwind the last two weeks ! Got a cold too that wo n't shift ? ? # sexy\nJoin me on FB for # Sneak # Peek Friday http://t.co/3DK5zyYQ1F for my upcoming # novel To Reap a Whirlwind # fiction # historical # wip # amwriting\nWAS Times : PawSox owners public return from whirlwind trip to Durham http://t.co/07nKMO7VaS\nI liked a @YouTube video http://t.co/a5YTAw9Vih S.O.S. Rona Guide - The Red Whirlwind\n[ VID]150806 Luhan From Luhan Studio Channel on YOUKU Update \n\n http://t.co/W7yeZkQlCJ http://t.co/C0QWZ0IshR\n{ { whirlwind romance dress | in - store & amp ; online } } # freshoutofthebox # getitbeforeitsgone # under50 # newnewnew ... http://t.co/8BUTsC4ZyN\n# HarperANetflixShow   Blazing Saddles   ( a show about wild fires in Alberta )\n? ? # BREAKING : massive wild- and field fires reported throughout Germany ! The most fires are in eastern # German http://t.co/4zSIy7EO2O\nWe all complain about PA but atleast we do n't have those wild fires taking our homes like north Cali or the floods in Florida right now\nmy wife has opted to take holidays & amp ; historically since we met @lanahillman gets special treatment her bonfires ca n't start wild fires - RAIN !\nLike blazing wild fires singing Your name !\nNetNewsLedger Wild Fire Update ÛÒ August 4 2015 : THUNDER BAY ÛÒ WEATHER ÛÒ ThereåÊwere no new fires confirmed by t ... http://t.co/JflxgEmBdA\n@myralynn24 Becuase it can cause property damage and cause wild fires\nHey @Macys ! My moms house burned down in the CA wild fires & amp ; I bought her an e gift card to buy her CLOTHES ( cont ) http://t.co/sWdjnypCXK\nCalifornia wild fires ... clearly caused by global climate change and excess CO2 ! ! ! ?\nCalifornia wild fires blow my mind every time\n@MrRat395 Are those wild fires getting anywhere near you ?\nStampedes and fires brought him home ; can her love make him stay ? TAME A WILD HEART # audiobook http://t.co/2dPKqAFtbr # BYNR @BeingAuthor\nGo home California you 're drunk . Natural selection has taken affect . Debt extreme wild fires water supply problems . You fail as a state .\n@CBS evening news covers tent collapse wild fires NO mention of worst Planned Parenthood video to date # Narrative\nWILD FIRES ! http://t.co/EgrMdkXpOi\nWild Fires In California Continue To Burn - http://t.co/nKcmib9WL0 http://t.co/CpMGBWVDXM\none of them wild fires in cali or something or in the mid west or where ever they show up .. shit do nt pop up around me\n@JacobHoggard @therealmattleaf   it 's so sad out there in BC all the wild fires . Hope u are safe .\nwhen there 's 35 wild fires in the US and 21 of them are in California .... AND we 're in a drought ..... this state is so sad\n11:57am Wildfire by The Mynabirds from Lovers Know\nFinger Rock wildfire grows to 500 acres # Tucson http://t.co/zSXGEI1Nxf\n# chattanooga The Latest : California governor headed to site of wildfire http://t.co/D6E7IICW5W\nAbout to get it with the babes of # wildfire and @taviiikenney of course ! !\nHere 's a quick timelapse I made of the Finger Rock Fire last night from about 9PM - 1AM . Check it out ! # fingerrockfire # wildfire # catalinas\n# WAwildfire in # chelan ? Wolverine fire results in evacuation . http://t.co/yePlnZPoWu\nca n't eat a wildfire chicken salad without thinking of / missing @Alynacarol ? ?\nCherokee Road and Road 22 ( Three Sisters Wildfire ) : There are two roads closed to the general public : ... http://t.co/WwnL8FvfMM # IDFire\nAs California fires rage the Forest Service sounds the alarm about sharply rising wildfire costs http://t.co/ht8FyiMJlR\nBe careful out there . http://t.co/KoBavdEKWn\nWolverine Fire Update - Thursday August 6 - 9:00 Am \n\n Incident : Wolverine Fire Wildfire \n Released : 41 min . ago ... http://t.co/8WDTTzpTXH\nCooler weather helps crews battling state wildfire .. Related Articles : http://t.co/15fHVIi9Qp\nWildfire near Columbia River town is 50 percent contained - http://t.co/yTPiPXpqr9 http://t.co/HCUQ6jpBtL\nsmHRN mhtw4fnet \n\n Crews gaining on huge Northern California wildfire - CBS News\nRocky Fire # cali # SCFD # wildfire # LakeCounty https://t.co/3pu0000v5o\nThis machine really captured my attention . # helicopter # firefighting # wildfire # oregon # easternoregonÛ _ https://t.co/eKt456jY1s\nForest Grove grass fire sparked by tractor quickly contained - KATU http://t.co/FMbcocS79u\nSan Patricio ÛÓ Windstorm insurance reform key for coastal homeowners : PORTLAND ÛÒ State Rep. J.M. Lozano spoke ... http://t.co/GQySVKrAGi\nTexas Seeks Comment on Rules for Changes to Windstorm Insurer http://t.co/SDZLVBiFbN\nNew roof and hardy going up ... Windstorm inspection tomorrow .. http://t.co/3XMLQRpNQR\nInsurance News : Texas Seeks Comment on Rules for Changes to Windstorm Insurer\nTWIA board approves 5 percent rate hike : The Texas Windstorm Insurance Association ( TWIA ) Board of Directors v ... http://t.co/NvxLJDsNkX\nGLORIA JONES - VIXEN / WINDSTORM ( 2IN1 ) CD 20 TRACKS MARC BOLAN http://t.co/E3cAymmIcN http://t.co/eRzcXloPDL\n@t_litt44 I seen you that one day look like you went threw a windstorm ? ?\nGreat time group camping at Presqu'ile with family&amp;friends . Lots of fun;not much sleep # PQPHistoryWeekend # windstorm http://t.co/0tYkaZyQpF\nTexas Seeks Comment on Rules for Changes to Windstorm Insurer : The Texas Department of Insurance is seeking pu ... http://t.co/NGF9n8Jquu\nA few more photos from this mornings windstorm . We 've never seen anything like it ! # rhodeisland # warwick\nPolice Officer Wounded Suspect Dead After Exchanging Shots http://t.co/e4wvXQItV4\nThe South Korean army wounded 44 persons ' Takeshima 's Japanese fisherman and occupies the island .  \n http://t.co/mJCXgKU8Yt\nGunmen kill four in El Salvador bus attack : Suspected Salvadoran gang members killed four people and wounded s ... http://t.co/LZWRONbTmi\nYou still acting like you were the one wounded . Did n't you do the stabbing ? ...\n# Dime_Miloko Officer Wounded Suspect Killed in Exchange of Gunfire : Richmond police officer wounded suspect killed in exchange of gunfire\nPolice officer wounded suspect dead after exchanging shots : A Richmond police officer was wounded and a suspe ... http://t.co/FzQPiQeCHB\nPope Francis talks about WOUNDED FAMILIES \n http://t.co/m1RfBPODI9\n@lordjewcup @helloimivan wounded warrior\nMan seriously wounded in Auburn Gresham shooting http://t.co/qPsSbAfSuL # Chiraq\nSuspected Salvadoran gang members killed four people and wounded seven more on Wednesday in an attack on a bus as it travelled down a ruralÛ _\nOfficer Wounded Suspect Killed in Exchange of Gunfire http://t.co/HHSjnAVHUA\n' Police Officer Wounded Suspect Dead After Exchanging Shots ' http://t.co/guNq7ZTUn4 # ? ? ? ? _ ? ? ? ? ?\nPolice Officer Wounded Suspect Dead After Exchanging Shots : Richmond police officer wounded suspect killed a ... http://t.co/YIKQRqczVZ\nHe was crushed for our iniquities ; the punishment that brought us peace was upon him & amp ; by his wounds we are healed . -Isa 53:5\n@bkanioros Lizeth and I tried over and over again to fix our relationship it never worked bk you can not heal those wounds I love you for\nLove wounds\n' @GoogleFacts : Wounds heal faster if you lick them . Tears also help them heal as well . '   I know right even thou it seems stupid when lickin\nthe wounds of honor \n  are self - inflicted\nME : gun shot wounds 3 4 6 7 ' rapidly lethal ' would have killed in 30 - 60 seconds or few minutes max . # kerricktrial\n@Cash7Nigga whoa . Insomniac . Why do n't you try to get some sleep . Sleep heals all wounds . Food too ! Night :)\nDo You Let Your Emotional Wounds Fester ? http://t.co/yDWhMgIMOE ? Powerful story andåÊword picture . via @forgivenwife\nIsraeli military shoot Palestinian who injured 3 in West Bank car attack http://t.co/P9DCnuLODR via @YahooNews\n@WorldRunners @RunningRoom Day 3&amp;my wounds are looking much better . Running road rash . # summer # injury # offdishduty http://t.co/aGkWKaqgOS\nTime heals all wounds . And if it does n't you name them something other than wounds and agree to let them stay . ? ? ? ?\n2 Policemen receive bullet wounds Reports indicate 6 - 7 armed men attacked a police post .\nPalestinian rams car into Israeli soldiers wounding 3 http://t.co/n79BMXQKlg # EMM\nand the salt in my wounds is nt burnin any more than it used to its not that i do nt feel the pain its just i m not afraid of hurting anymore\nWhen and how does a character recover fromåÊwounds ? http://t.co/ohhkuHtjXm\nWhat they ( dentists ) do n't tell u is how much ur mouth will be in pain when the numbing wears off because of the puncture wounds in ur gums .\nOur wounds can so easily turn us into people we do n't want to be and we hardly see it happening\ni 'm sitting in the parking lot waiting to go into therapy and i 'm crying and an emotional wreck and i do n't want my therapist to know ? ? ? ? ? ? ? ? ? ? ? ?\nLast week we had a blast hosting Dinner & amp ; a Movie Night with ' Wreck - It Ralph ' on @FtCarsonPAO ! @carsonmwr # TBT http://t.co/tUYPxEoBQa\nNow that IÛªve figured out how to get my music in my rental car I can take a night drive to nowhere tonight . Hopefully without a wreck .\ni 'm an emotional wreck watching emmerdale fml\nInteresting in watching a train wreck while taking acid kind of way https://t.co/kM08qiWW4 g\n@DukeSkywalker @facialabuse you should do a competetion between @xxxmrbootleg & amp ; # ClaudioMeloni ( ultimate throat penetrator ) to a wreck off .\nMan faces manslaughter charges following fatal Sunday wreck | AL.c .. Related Articles : http://t.co/5xrFhdXTvX\n* is a wreck * * gives ppl lifestyle advice *\nI 'm an emotional wreck someone hold me until they upload the damn video # whatstheimportantvideo\nI added a video to a @YouTube playlist http://t.co/wYtu7vIwsj Burnt Black - Nervous Wreck\nand not a wreck of uneven layers   https://t.co/Y0WE0wXQCp\nDj d wreck cut the beat\n@Herologist i know right ? It 's a train wreck . You ca n't believe it 's happening but you ca n't look away .\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM : Investigators and the families of those who were ... http://t.co/cUrJAT1BqI\nWreckage ' conclusively confirmed ' as from MH370 : Malaysia PM http://t.co/Se1QkNPltS | https://t.co/DnhRviV1dJ\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM : Investigators and the families of those who were ... http://t.co/O8sTW4YN8R\n# science Now that a piece of wreckage from flight MH370 has been confirmed on RÌ © union Island is it possible t ...   http://t.co/cKQ7tOE82u\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM : Investigators and the families of those who were ... http://t.co/1RzskdRrDk\nTOP STORY : wreckage from # MH370 officially confirmed . @janisctv reports on the critical clue from a tiny island . http://t.co/hlWw2P5k9o\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM http://t.co/MN130C4e2D via @ndtv\nWreckage ' Conclusively Confirmed ' as From MH370 : Malaysia PM : Investigators and the families of those who were ... http://t.co/mzB0MKuUo7\nRT @australian Debris found on an Indian Ocean island confirmed to be from flight # MH370 . http://t.co/gY9MrSl6x2\nCramer : Iger 's 3 words that wrecked Disney 's stock http://t.co/4dGpBAiVL7\nAlmost * wrecked * my van the other day because of this guy ( yeah I brake & amp ; also care for animals ~ get used to it ) !   https://t.co/8ZaHavmtT6\n' What manner of human being would parcel out a baby as though it were a wrecked car in a junk yard ? TheÛ _ ' ÛÓ dtom2 http://t.co/lj6lzo4isX\n@Nathan26_RFC thought you said Saturday night there and near died lolol you 'll be wrecked ! !\nI just wanna ease your mind and make you feel alright . ? ? ? ?\n@yakubOObs think he deactivated because his notifications are aids after tesco wrecked him lol\nRT CNBC ' 3 words from Disney CEO Bob Iger wrecked Disney 's stock says Jim Cramer : http://t.co/f0texKsqhL http://t.co/ilySLaTMgI '\nSmackdown tyme this should put me in a good mood again since it got wrecked smh\n@thrillhho jsyk I have n't stopped thinking abt remus slumped against the bathroom door all day I was wrecked ? ? ? ? ? ? ? ? ? ?\n@stighefootball Begovic has been garbage . He got wrecked by a Red Bull reserve team and everyone else this preseason\nWrecked today got my hattrick ? ? ? ?\n# Ebola # EbolaOutbreak Ebola Virus : Birmingham Ala. Firefighters Quarantined After Possible Exposure Officials Say http://t.co/tjpYlU9fOX\nMalaysian PM confirms debris is from missing flight MH370 http://t.co/pfAvW5QyqE\nOfficials : Alabama home quarantined over possible Ebola case - Washington Times\nSee the 16yr old PKK suicide bomber who detonated bomb in Turkey Army trench released : Harun Ìàekdar ... http://t.co/hKuT5mSdtP @MsOreo _\nTo conference attendees ! The blue line from the airport has DERAILED - please look into taking a taxi to the hotel ! See you soon !\nThe death toll in a # IS - suicide car bombing on a # YPG position in the Village of Rajman in the eastern province of Hasaka has risen to 9\nEARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn\nStorm in RI worse than last hurricane . My city&amp;3others hardest hit . My yard looks like it was bombed . Around 20000 K still without power\nGreen Line derailment in Chicago http://t.co/UtbXLcBIuY\nMEG issues Hazardous Weather Outlook ( HWO ) http://t.co/3X6RBQJHn3\n# CityofCalgary has activated its Municipal Emergency Plan . # yycstorm\n"
    },
    {
     "data": {
      "text/plain": "[1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n ...]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for i in range(len(test_loader.examples)):\n",
    "    text = ' '.join(vars(test_loader.examples[i])['text'])\n",
    "    print(text)\n",
    "    if predict(model, text) > .7:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3263"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id  target\n0   0       1\n1   2       0\n2   3       0\n3   9       0\n4  11       1\n5  12       0\n6  21       0\n7  22       0\n8  27       0\n9  29       0"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(data_dir + \"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}